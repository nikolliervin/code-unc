# Default configuration for AI Code Review CLI
# Copy this file and customize for your needs

# AI Provider Configuration
ai:
  provider: "ollama"  # Options: openai, anthropic, gemini, ollama
  model: "tinyllama"  # Model name - provider specific
  temperature: 0.1    # 0.0 = deterministic, 1.0 = creative
  max_tokens: 4000    # Maximum tokens to generate
  
  # OpenAI Settings (when provider = "openai")
  openai_api_key: null  # Set via OPENAI_API_KEY environment variable
  openai_base_url: null # Optional custom base URL
  
  # Anthropic Settings (when provider = "anthropic") 
  anthropic_api_key: null  # Set via ANTHROPIC_API_KEY environment variable
  
  # Google Gemini Settings (when provider = "gemini")
  gemini_api_key: null  # Set via GEMINI_API_KEY environment variable
  gemini_base_url: null # Optional custom base URL
  
  # Ollama Settings (when provider = "ollama")
  ollama_base_url: "http://localhost:11434"
  ollama_model: "tinyllama"
  
  # Retry Configuration
  max_retries: 3
  retry_delay: 1.0

# Review Configuration  
review:
  max_files: 50
  include_patterns: []
  exclude_patterns: 
    - "*.log"
    - "*.tmp"
    - "node_modules/*"
    - ".git/*"
  focus_areas:
    - "general"
    - "security" 
    - "performance"
    - "style"

# Output Configuration
output:
  format: "rich"  # Options: rich, json, markdown, html
  show_progress: true
  show_metrics: true
  show_suggestions: true
  max_issues_display: 50
  console_width: null  # Auto-detect if null
  color_enabled: true

# Git Configuration
git:
  ignore_whitespace: false
  ignore_binary: true
  max_diff_size: 1000000  # 1MB limit
  
# Cache Configuration  
cache:
  enabled: true
  git_diff_ttl: 3600      # 1 hour
  ai_response_ttl: 86400  # 24 hours
  max_size_mb: 100        # Maximum cache size 